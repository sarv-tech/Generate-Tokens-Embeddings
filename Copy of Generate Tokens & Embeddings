{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1s4dXnuV6i0zq1CqxLpeeF9gEMBGzRxcB","timestamp":1756752508715}],"authorship_tag":"ABX9TyPP241AXYWEy//X+d1kgj3V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"13ys7ZdXduR4","executionInfo":{"status":"ok","timestamp":1756751820805,"user_tz":-330,"elapsed":29458,"user":{"displayName":"Sarvesh Pingale","userId":"08327096150766188309"}},"outputId":"fcb892cd-a22c-430b-88b5-8011df3afeb6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n","Enter a sentence to generate tokens and embeddings: Hi, how are you?\n","\n","Tokens: ['Hi', ',', 'Ġhow', 'Ġare', 'Ġyou', '?']\n","\n","Token Embeddings (first 5 dimensions): \n","  Token                     Embedding (first 5 dimensions)\n","0    Hi  [-0.05105082, -0.19037676, -0.33829486, -0.039...\n","1     ,  [0.37693793, 0.16349925, 0.2290436, -0.0467884...\n","2  Ġhow  [0.28224522, -0.38418308, -0.34325212, 0.36272...\n","3  Ġare  [-0.07760657, 0.4621003, 0.018556807, 0.823149...\n","4  Ġyou  [0.095505334, -0.31988353, -0.71785665, 0.3854...\n","5     ?  [0.25585005, -0.25344747, 0.0029500308, 0.0062...\n","\n","Embeddings saved to token_embeddings.csv\n"]}],"source":["# 1. Install the Transformers library\n","!pip install transformers\n","\n","# 2. Import required modules\n","import torch\n","import pandas as pd\n","from transformers import GPT2Tokenizer, GPT2Model\n","\n","# 3. Load the GPT-2 tokenizer and model\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","model = GPT2Model.from_pretrained('gpt2')\n","\n","# 4. Take user input\n","sentence = input(\"Enter a sentence to generate tokens and embeddings: \")\n","\n","# 5. Tokenize the input sentence\n","inputs = tokenizer(sentence, return_tensors='pt')\n","tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","print(\"\\nTokens:\", tokens)\n","\n","# 6. Generate embeddings for the input sentence\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    last_hidden_states = outputs.last_hidden_state\n","\n","# 7. Prepare embeddings for DataFrame (showing first 5 dimensions for simplicity)\n","embeddings = last_hidden_states[0].detach().numpy()\n","df = pd.DataFrame({\n","    \"Token\": tokens,\n","    \"Embedding (first 5 dimensions)\": [emb[:5] for emb in embeddings]\n","})\n","\n","# 8. Display the DataFrame\n","print(\"\\nToken Embeddings (first 5 dimensions): \")\n","print(df)\n","\n","# 9. Save to CSV\n","df.to_csv(\"token_embeddings.csv\", index=False)\n","print(\"\\nEmbeddings saved to token_embeddings.csv\")\n"]}]}